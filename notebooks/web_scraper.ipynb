{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8956f54d-2366-4eb7-a64f-07402ce2480d",
   "metadata": {},
   "source": [
    "# ðŸ‘œ Automated Job Data Extraction from Naukri.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd2463-6e73-4885-a4ca-521e6373cc9e",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525a55f-0d83-4ed4-9ad3-7ef3554e152e",
   "metadata": {},
   "source": [
    "To develop a job scraping tool that automatically extracts job listings based on user-defined keywords, roles, and locations. The extracted data is then analyzed to provide insights into job trends.\n",
    "\n",
    "> ***Objective :***\n",
    "* Automate job data collection from platforms like Naukri.com.\n",
    "* It allows users to input job roles, locations, and desired job counts dynamically.\n",
    "* Store scraped job data in an Excel file in data folder for further analysis.\n",
    "* An Streamlit web app for a user-friendly interface.\n",
    "* Perform basic data analysis to extract insights from collected job listings.\n",
    "\n",
    "> ***Success Criteria :***\n",
    "* Successfully scrapes job title, company, location, experience, salary, skills, job description, and job link from Naukri.com.\n",
    "* Handles missing values (e.g., \"Not Specified\" for missing salary).\n",
    "* Automatically creates the data/ folder if it doesnâ€™t exist.\n",
    "* Scrapes multiple pages (e.g., 1-5) and stops when no more jobs are found.\n",
    "* Confirms successful file saving in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463e083-4266-4941-b592-9c23c5129c93",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca844a6a-151f-4314-9906-9160a781de20",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facb291-4aab-4da8-af38-92b0ba2d0048",
   "metadata": {},
   "source": [
    "> ***Data Source :***\n",
    "* Job listings scraped from Naukri.com using Selenium.\n",
    "\n",
    "> ***Collected Features :***\n",
    "* Job Title\t\n",
    "* Company Name \n",
    "* Location \n",
    "* Experience\n",
    "* Salary\n",
    "* Job Description\n",
    "* Job Link \n",
    "\n",
    "> ***Challenges in Data Collection :***\n",
    "* Some Jobs may lack salary or experience details.\n",
    "* Pagination handling for fetching more job listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea6b0a9-01a7-4457-8ada-05da23258a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Job role (e.g., Machine Learning Engineer):  MongoDb\n",
      "Enter the Location (e.g., Bengaluru):  Bangalore\n",
      "Enter the total No. of the Jobs to Scrap:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "âœ… Scraping completed!\n",
      "âœ… All job listings saved to ../data\\MongoDb_naukri_jobs_with_details.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# User Inputs\n",
    "job_role = input(\"Enter the Job role (e.g., Machine Learning Engineer): \").replace(\" \", \"%20\")\n",
    "location = input(\"Enter the Location (e.g., Bengaluru): \").replace(\" \", \"%20\")\n",
    "max_jobs = int(input(\"Enter the total No. of the Jobs to Scrap: \"))\n",
    "\n",
    "\n",
    "# Setting up WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "base_url = f\"https://www.naukri.com/{job_role}-jobs-in-{location}?k={job_role}&l={location}\"\n",
    "driver.get(base_url)\n",
    "time.sleep(6)  # Allow page to load\n",
    "\n",
    "job_list = []\n",
    "job_count=0\n",
    "max_pages =  5 # pages to srape. Naukri.com has 21 job posting on single page\n",
    "\n",
    "for page in range(1, max_pages + 1):\n",
    "    print(f\"Scraping Page {page}...\")\n",
    "\n",
    "    jobs = driver.find_elements(By.CLASS_NAME, \"srp-jobtuple-wrapper\")\n",
    "\n",
    "    if not jobs:\n",
    "        print(f\" No jobs found on Page {page}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    for job in jobs:\n",
    "        if job_count >= max_jobs:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            title = job.find_element(By.CLASS_NAME, \"title\").text\n",
    "            company = job.find_element(By.CLASS_NAME, \"comp-name\").text\n",
    "            location = job.find_element(By.CLASS_NAME, \"loc\").text\n",
    "            experience = job.find_element(By.CLASS_NAME, \"exp\").text if job.find_elements(By.CLASS_NAME, \"exp\") else \"Not Specified\"\n",
    "            \n",
    "            #job link for naukri.com for specifiic job role\n",
    "            job_link = job.find_element(By.CLASS_NAME, \"title\").get_attribute(\"href\")  \n",
    "            \n",
    "            #Visiting the each job page directly to get particularr info\n",
    "            driver.execute_script(\"window.open();\")  # Open new tab\n",
    "            driver.switch_to.window(driver.window_handles[1])  # Switch to new tab\n",
    "            driver.get(job_link)  # Go to job link\n",
    "            time.sleep(4)\n",
    "\n",
    "        #Extracting job description\n",
    "            try:\n",
    "                job_desc = driver.find_element(By.CLASS_NAME, \"styles_JDC__dang-inner-html__h0K4t\").text[:400]  # Extract first 400 chars\n",
    "            except:\n",
    "                job_desc = \"Not Available\"\n",
    "\n",
    "            # Extracting the salary\n",
    "            try:\n",
    "                salary = driver.find_element(By.CLASS_NAME, \"styles_jhc__salary__jdfEC\").text\n",
    "            except:\n",
    "                salary = \"Not Specified\"\n",
    "\n",
    "            # Extracting the skills if available\n",
    "            try:\n",
    "                skills_container = driver.find_element(By.CLASS_NAME, \"styles_key-skill__GIPn_\")  \n",
    "                skills = [skill.text for skill in skills_container.find_elements(By.TAG_NAME, \"span\")]\n",
    "            except:\n",
    "                skills = [\"Not Specified\"]\n",
    "\n",
    "            # Closing job tab and switching back\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "            job_list.append([title, company, location, experience, salary, skills, job_desc, job_link])\n",
    "            job_count += 1\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Skipping job due to error:\", e)\n",
    "            \n",
    "    if job_count >= max_jobs:\n",
    "        break\n",
    "\n",
    "    # Moving to next page\n",
    "\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, f\"//a[text()='{page + 1}']\")  # Selecting the next page button\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page)  # Clicking using JavaScript\n",
    "        time.sleep(5)  # Wait for the next page to load\n",
    "    except Exception as e:\n",
    "        print(\"No more pages found or error:\", e)\n",
    "        break  # Stop scraping if no more pages are available\n",
    "\n",
    "print(\"âœ… Scraping completed!\")\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(job_list, columns=[\"Job Title\", \"Company\", \"Location\", \"Experience\", \"Salary\", \"Skills\", \"Job Description\", \"Job Link\"])\n",
    "\n",
    "# (one level up from notebooks/) datafolder\n",
    "data_folder = \"../data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "#Generating the filename\n",
    "filename = f\"{job_role.replace('%20', '_')}_naukri_jobs_with_details.xlsx\"\n",
    "file_path = os.path.join(data_folder, filename)\n",
    "\n",
    "# Saving DataFrame to Excel using full path\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"âœ… All job listings saved to {file_path}\")\n",
    "\n",
    "# browser =closed\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41fc87-a5a3-4a33-aaac-ca13f795f7db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
